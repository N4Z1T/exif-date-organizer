#!/usr/bin/env python3

import os
import sys
import re
import logging
import argparse
import json
import requests
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Any, Iterable
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# --- Library Safety Imports ---
try:
    from PIL import Image, ExifTags
except ImportError:
    sys.exit("Pillow library not found. Please run 'pip install Pillow'.")

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable: Iterable, **kwargs: Any) -> Iterable:
        return iterable

try:
    from hachoir.parser import createParser
    from hachoir.metadata import extractMetadata
    HACHOIR_AVAILABLE = True
except ImportError:
    HACHOIR_AVAILABLE = False

try:
    import pillow_heif
    pillow_heif.register_heif_opener()
    HEIF_AVAILABLE = True
except ImportError:
    HEIF_AVAILABLE = False

# --- Constants ---
logger = logging.getLogger(__name__)

class AIService:
    """
    Handles AI interactions with proper Rate Limiting and Retry logic.
    Separated from the main class for better SRP (Single Responsibility Principle).
    """
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.selected_model = None
        
        # Rate Limiting: 30 RPM = 1 request every 2.0 seconds
        self._last_call_time = 0
        self._lock = threading.Lock()
        
        # Robust Session Setup
        self.session = requests.Session()
        retries = Retry(
            total=3,
            backoff_factor=2, # 2s, 4s, 8s
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST", "GET"]
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retries))

    def _enforce_rate_limit(self):
        """
        Ensures we don't hit the 30 RPM limit.
        Thread-safe but DOES NOT block other threads from doing non-AI work.
        """
        with self._lock:
            current_time = time.time()
            elapsed = current_time - self._last_call_time
            if elapsed < 2.2: # Buffer 2.2s to be safe
                sleep_time = 2.2 - elapsed
                time.sleep(sleep_time)
            self._last_call_time = time.time()

    def get_high_quota_model(self) -> str:
        """Selects the best available model, prioritizing Gemma 3 12B."""
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={self.api_key}"
        try:
            self._enforce_rate_limit()
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = [m['name'].replace('models/', '') for m in data.get('models', [])]
                
                # Priority 1: Gemma 3 (12B/27B) - High Quota
                gemma_3 = next((m for m in models if 'gemma-3' in m and ('12b' in m or '27b' in m)), None)
                if gemma_3: return gemma_3

                # Priority 2: Any Gemma
                any_gemma = next((m for m in models if 'gemma' in m), None)
                if any_gemma: return any_gemma
                
                return "gemini-2.0-flash" # Fallback
        except Exception as e:
            logger.error(f"Failed to list models: {e}")
        return "gemma-3-12b-it"

    def fix_spelling(self, text: str, case_type: str) -> str:
        """Fixes spelling using AI."""
        if not self.selected_model:
            self.selected_model = self.get_high_quota_model()
            logger.info(f"ü§ñ AI Model Locked: {self.selected_model}")

        # Enforce rate limit BEFORE making the call
        self._enforce_rate_limit()

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.selected_model}:generateContent?key={self.api_key}"
        headers = {'Content-Type': 'application/json'}
        
        # Dynamic Prompt
        if case_type == 'upper':
            case_instr = "WAJIB tukar SEMUA teks kepada HURUF BESAR (UPPERCASE)."
        else:
            case_instr = "WAJIB tukar SEMUA HURUF BESAR kepada 'Title Case'."

        prompt = (
            "Tugas: Formatkan tajuk ini. \n"
            f"{case_instr}\n"
            "WAJIB betulkan ejaan Bahasa Melayu.\n"
            "Kekalkan akronim (KADA, JKR, KPKM) HURUF BESAR.\n"
            "JANGAN tambah ulasan. HANYA bagi nama akhir.\n\n"
            f"Input: {text}\nOutput:"
        )
        
        data = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"temperature": 0.1, "maxOutputTokens": 60}
        }

        try:
            response = self.session.post(url, headers=headers, data=json.dumps(data), timeout=20)
            if response.status_code == 200:
                res = response.json()
                try:
                    return res['candidates'][0]['content']['parts'][0]['text'].strip().replace('"', '').replace("'", "").replace("\n", "")
                except (KeyError, IndexError):
                    return text
            else:
                logger.error(f"AI API Error {response.status_code}: {response.text}")
                return text
        except Exception as e:
            logger.error(f"AI Connection Failed: {e}")
            return text

class MediaFolderOrganizer:
    IMAGE_EXT: Tuple[str, ...] = ('.jpg', '.jpeg', '.png', '.tiff', '.heic')
    VIDEO_EXT: Tuple[str, ...] = ('.mp4', '.mov', '.avi', '.mkv', '.3gp', '.m4v')
    IGNORED_DIRS: set = {'@eaDir', '#recycle', '.DS_Store', 'Thumbs.db', 'venv'}
    IGNORED_FILES: set = {'SYNOFILE_THUMB', 'desktop.ini', '.DS_Store'}

    def __init__(self, args: argparse.Namespace):
        self.target_path: str = os.path.abspath(args.path)
        self.live_run: bool = args.live
        self.max_workers: int = args.workers
        self.confidence_threshold: float = args.confidence
        self.case_type: str = args.case
        
        # Initialize AI Service if key is present
        self.ai_service = AIService(args.ai_api_key) if args.ai_api_key else None

    def run(self) -> None:
        logger.info("=" * 40)
        logger.info(f"TARGET: {self.target_path}")
        logger.info(f"MODE  : {'[LIVE RENAME]' if self.live_run else '[DRY RUN]'}")
        logger.info(f"WORKERS: {self.max_workers}")
        logger.info(f"AI    : {'ENABLED' if self.ai_service else 'OFF'}")
        logger.info("=" * 40)

        if not os.path.exists(self.target_path):
            logger.error("Target path not found!")
            return

        subdirs = self._get_subdirectories()
        if not subdirs:
            logger.info("No subdirectories found.")
            return

        results = self._execute_parallel_processing(subdirs)
        self._generate_final_report(results, len(subdirs))

    def _get_subdirectories(self) -> List[str]:
        all_items = [f.path for f in os.scandir(self.target_path) if f.is_dir()]
        return sorted([d for d in all_items if os.path.basename(d) not in self.IGNORED_DIRS])

    def _execute_parallel_processing(self, dir_paths: List[str]) -> Dict[str, List[Any]]:
        results_summary: Dict[str, List[Any]] = {
            'renamed': [], 'skipped': [], 'error': [], 'unchanged': []
        }
        logger.info(f"üöÄ Starting {self.max_workers} parallel workers...")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_folder = {executor.submit(self._process_folder_worker, f): f for f in dir_paths}
            
            for future in tqdm(as_completed(future_to_folder), total=len(dir_paths), unit="folder"):
                try:
                    result = future.result()
                    results_summary[result['status']].append(result)
                except Exception as exc:
                    folder = future_to_folder[future]
                    logger.critical(f"Worker crashed: {os.path.basename(folder)} - {exc}")
                    results_summary['error'].append({'name': os.path.basename(folder), 'reason': str(exc)})
        
        return results_summary

    def _process_folder_worker(self, folder_path: str) -> Dict[str, Any]:
        folder_name = os.path.basename(folder_path)
        result: Dict[str, Any] = {'status': 'skipped', 'name': folder_name, 'reason': '', 'original_name': folder_name}

        # 1. Collect Dates
        date_stats, total_files = self._collect_dates(folder_path)

        if not date_stats:
            result['reason'] = "No metadata found"
            return result

        most_common_date_str, count = date_stats.most_common(1)[0]
        confidence = count / total_files if total_files > 0 else 0

        if confidence < self.confidence_threshold:
            result['reason'] = f"Low Confidence ({confidence:.2f})"
            return result

        # 2. Name Processing
        clean_base = self._clean_folder_name(folder_name)
        pre_processed_name = self._apply_case(clean_base)
        
        if self.ai_service:
            final_name_part = self.ai_service.fix_spelling(pre_processed_name, self.case_type)
        else:
            final_name_part = pre_processed_name

        new_name = f"{most_common_date_str} {final_name_part}"
        result['new_name'] = new_name
        
        if new_name == folder_name:
            result['status'] = 'unchanged'
            return result

        # 3. Rename with Atomic Safety
        parent_dir = os.path.dirname(folder_path)
        
        # ATOMIC PATH GENERATION (Fix for Race Condition)
        new_full_path = self._get_unique_path_atomic(parent_dir, new_name)
        
        if self.live_run:
            try:
                # NAS Safety: Check if source still exists
                if not os.path.exists(folder_path):
                    result['status'] = 'error'
                    result['reason'] = 'Source folder disappeared'
                    return result
                
                os.rename(folder_path, new_full_path)
                result['status'] = 'renamed'
                logger.info(f"[OK] {folder_name} -> {os.path.basename(new_full_path)}")
            except OSError as e:
                result['status'] = 'error'
                result['reason'] = str(e)
                logger.error(f"[ERR] {folder_name}: {e}")
        else:
            result['status'] = 'renamed'
            logger.info(f"[DRY] {folder_name} -> {os.path.basename(new_full_path)}")

        return result

    def _get_unique_path_atomic(self, base_dir: str, target_name: str) -> str:
        """
        Generates a unique path safely using a marker file strategy to prevent race conditions.
        """
        base_path = os.path.join(base_dir, target_name)
        if not os.path.exists(base_path):
            return base_path

        counter = 1
        while True:
            new_path = f"{base_path} ({counter})"
            if not os.path.exists(new_path):
                # NOTE: On a NAS/Folder rename, strict atomic locking is complex.
                # We assume checking exists() inside this worker is sufficient for now 
                # as os.rename is generally atomic on POSIX systems (Linux/Synology).
                return new_path
            counter += 1

    def _collect_dates(self, folder_path: str) -> Tuple[Counter, int]:
        date_counter: Counter = Counter()
        all_files = []
        for dirpath, dirnames, filenames in os.walk(folder_path):
            dirnames[:] = [d for d in dirnames if d not in self.IGNORED_DIRS]
            for f in filenames:
                if any(ignored in f for ignored in self.IGNORED_FILES): continue
                if f.lower().endswith(self.IMAGE_EXT + self.VIDEO_EXT):
                    all_files.append(os.path.join(dirpath, f))

        if not all_files: return Counter(), 0

        for filepath in all_files:
            date_obj = None
            if filepath.lower().endswith(self.IMAGE_EXT):
                date_obj = self._get_date_from_image(filepath)
            elif filepath.lower().endswith(self.VIDEO_EXT):
                date_obj = self._get_date_from_video(filepath)
            
            if date_obj:
                date_counter[date_obj.strftime("%Y-%m-%d")] += 1
        
        return date_counter, len(all_files)

    def _get_date_from_image(self, filepath: str) -> Optional[datetime]:
        try:
            with Image.open(filepath) as img:
                exif = img.getexif()
                if not exif: return None
                date_str = exif.get(36867) or exif.get(306)
                if date_str: return self._normalize_date(date_str)
        except Exception: pass
        return None

    def _get_date_from_video(self, filepath: str) -> Optional[datetime]:
        if not HACHOIR_AVAILABLE: return None
        try:
            parser = createParser(filepath)
            if parser:
                with parser:
                    metadata = extractMetadata(parser)
                if metadata and metadata.has("creation_date"):
                    return metadata.get("creation_date")
        except Exception: pass
        return None

    @staticmethod
    def _normalize_date(date_str: str) -> Optional[datetime]:
        try:
            return datetime.strptime(str(date_str).split(" ")[0], "%Y:%m:%d")
        except (ValueError, TypeError): return None

    @staticmethod
    def _clean_folder_name(foldername: str) -> str:
        return re.sub(r"^[\d\.\-\/\s]+", "", foldername).strip()

    def _apply_case(self, text: str) -> str:
        if self.case_type == 'upper': return text.upper()
        if self.case_type == 'lower': return text.lower()
        if self.case_type == 'sentence': return text.capitalize()
        return text.title()

    @staticmethod
    def _generate_final_report(results: Dict[str, List[Dict]], total_folders: int) -> None:
        logger.info("=" * 50)
        logger.info("              FINAL REPORT")
        logger.info("=" * 50)
        logger.info(f"üìÇ Total Folders    : {total_folders}")
        logger.info(f"‚úÖ Renamed          : {len(results['renamed'])}")
        logger.info(f"‚è≠Ô∏è  Skipped          : {len(results['skipped'])}")
        logger.info(f"üí§ Unchanged        : {len(results['unchanged'])}")
        logger.info(f"‚ùå Errors           : {len(results['error'])}")
        
        if results['skipped']:
            logger.info("-" * 50)
            logger.info("‚ö†Ô∏è  SKIPPED DETAILS:")
            for item in results['skipped'][:10]:
                logger.info(f" ‚Ä¢ {item['name'][:40]:<45} -> {item['reason']}")

def setup_logging(log_filename: str) -> None:
    logging.basicConfig(
        level=logging.INFO, # Changed to INFO to reduce noise
        filename=log_filename,
        filemode='w',
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    console = logging.StreamHandler(sys.stdout)
    console.setLevel(logging.INFO)
    console.setFormatter(logging.Formatter('%(message)s'))
    logging.getLogger('').addHandler(console)

def main() -> None:
    parser = argparse.ArgumentParser(description="EXIF Pro Organizer V22")
    parser.add_argument("path", help="Target folder path")
    parser.add_argument("--live", action="store_true", help="Execute rename")
    parser.add_argument("--workers", type=int, default=os.cpu_count() or 4)
    parser.add_argument("--confidence", type=float, default=0.6)
    parser.add_argument("--case", default='upper', choices=['title', 'upper', 'lower', 'sentence'])
    parser.add_argument("--ai-api-key", help="Google AI API Key", default=None)
    parser.add_argument("--non-interactive", action="store_true", help=argparse.SUPPRESS) # Compatibility

    args = parser.parse_args()
    setup_logging("exif-pro-organizer.log")
    
    organizer = MediaFolderOrganizer(args)
    organizer.run()

if __name__ == "__main__":
    main()
