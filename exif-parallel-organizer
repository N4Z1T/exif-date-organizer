#!/usr/bin/env python3

import os
import sys
import re
import logging
import argparse
import json
import requests
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Any, Iterable
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# --- Library Safety Imports ---
try:
    from PIL import Image, ExifTags
except ImportError:
    sys.exit("Pillow library not found. Please run 'pip install Pillow'.")

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable: Iterable, **kwargs: Any) -> Iterable:
        return iterable

try:
    from hachoir.parser import createParser
    from hachoir.metadata import extractMetadata
    HACHOIR_AVAILABLE = True
except ImportError:
    HACHOIR_AVAILABLE = False

try:
    import pillow_heif
    pillow_heif.register_heif_opener()
    HEIF_AVAILABLE = True
except ImportError:
    HEIF_AVAILABLE = False

# --- Configuration Constants ---
INVALID_FILENAME_CHARS = r'[<>:"/\\|?*]'
IMAGE_EXT = ('.jpg', '.jpeg', '.png', '.tiff', '.heic')
VIDEO_EXT = ('.mp4', '.mov', '.avi', '.mkv', '.3gp', '.m4v')
IGNORED_DIRS = {'@eaDir', '#recycle', '.DS_Store', 'Thumbs.db', 'venv', '.git'}
IGNORED_FILES = {'SYNOFILE_THUMB', 'desktop.ini', '.DS_Store'}

logger = logging.getLogger(__name__)

# ==========================================
# MODULE 1: AI SERVICE (Network I/O)
# ==========================================
class AIService:
    """Handles interaction with Google Gemini API with Rate Limiting."""
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.selected_model = None
        self._last_call_time = 0
        self._lock = threading.Lock()
        
        self.session = requests.Session()
        retries = Retry(total=3, backoff_factor=2, status_forcelist=[429, 500, 502, 503])
        self.session.mount("https://", HTTPAdapter(max_retries=retries))

    def _enforce_rate_limit(self):
        """Ensures 30 RPM (1 request per ~2s)."""
        with self._lock:
            current_time = time.time()
            elapsed = current_time - self._last_call_time
            if elapsed < 2.2:
                time.sleep(2.2 - elapsed)
            self._last_call_time = time.time()

    def get_model(self) -> str:
        """Finds the best model (Gemma 3) or falls back."""
        if self.selected_model: return self.selected_model
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={self.api_key}"
        try:
            self._enforce_rate_limit()
            res = self.session.get(url, timeout=10)
            if res.status_code == 200:
                models = [m['name'].replace('models/', '') for m in res.json().get('models', [])]
                # Priority: Gemma 3 -> Any Gemma -> Gemini Flash
                self.selected_model = next((m for m in models if 'gemma-3' in m and ('12b' in m or '27b' in m)),
                                      next((m for m in models if 'gemma' in m), "gemini-2.0-flash"))
            else:
                self.selected_model = "gemma-3-12b-it" # Optimistic default
        except Exception:
            self.selected_model = "gemma-3-12b-it"
            
        logger.info(f"ü§ñ AI Model Selected: {self.selected_model}")
        return self.selected_model

    def fix_spelling(self, text: str, case_type: str) -> str:
        model = self.get_model()
        self._enforce_rate_limit()

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={self.api_key}"
        case_instr = "WAJIB tukar SEMUA teks kepada HURUF BESAR." if case_type == 'upper' else "WAJIB tukar HURUF BESAR kepada 'Title Case'."
        
        prompt = (
            f"Tugas: Formatkan tajuk folder ini.\n{case_instr}\n"
            "WAJIB betulkan ejaan Bahasa Melayu.\nKekalkan akronim (KADA, JKR) HURUF BESAR.\n"
            f"Input: {text}\nOutput:"
        )

        try:
            res = self.session.post(url, headers={'Content-Type': 'application/json'}, 
                                  data=json.dumps({"contents": [{"parts": [{"text": prompt}]}]}), timeout=20)
            if res.status_code == 200:
                return res.json()['candidates'][0]['content']['parts'][0]['text'].strip().replace('"', '').replace("'", "").replace("\n", "")
        except Exception as e:
            logger.debug(f"AI Error: {e}")
        
        return text

# ==========================================
# MODULE 2: METADATA SCANNER (Disk Read)
# ==========================================
class MetadataScanner:
    """Handles file scanning and date extraction."""
    
    def scan_folder(self, folder_path: str) -> Tuple[Counter, int]:
        date_counter = Counter()
        files_found = []
        
        for dirpath, dirnames, filenames in os.walk(folder_path):
            dirnames[:] = [d for d in dirnames if d not in IGNORED_DIRS]
            for f in filenames:
                if any(i in f for i in IGNORED_FILES): continue
                if f.lower().endswith(IMAGE_EXT + VIDEO_EXT):
                    files_found.append(os.path.join(dirpath, f))

        if not files_found: return Counter(), 0

        # Scan files (Limit logging to avoid I/O spam)
        for filepath in files_found:
            date_obj = self._get_date(filepath)
            if date_obj:
                date_counter[date_obj.strftime("%Y-%m-%d")] += 1
                
        return date_counter, len(files_found)

    def _get_date(self, filepath: str) -> Optional[datetime]:
        ext = os.path.splitext(filepath)[1].lower()
        if ext in IMAGE_EXT:
            return self._get_image_date(filepath)
        elif ext in VIDEO_EXT:
            return self._get_video_date(filepath)
        return None

    def _get_image_date(self, filepath: str) -> Optional[datetime]:
        try:
            with Image.open(filepath) as img:
                exif = img.getexif()
                if not exif: return None
                date_str = exif.get(36867) or exif.get(306)
                return self._parse_date(date_str)
        except Exception: return None

    def _get_video_date(self, filepath: str) -> Optional[datetime]:
        if not HACHOIR_AVAILABLE: return None
        try:
            parser = createParser(filepath)
            if parser:
                with parser:
                    meta = extractMetadata(parser)
                    if meta and meta.has("creation_date"):
                        return meta.get("creation_date")
        except Exception: return None

    @staticmethod
    def _parse_date(date_str: Any) -> Optional[datetime]:
        try:
            return datetime.strptime(str(date_str).split(" ")[0], "%Y:%m:%d")
        except (ValueError, TypeError): return None

# ==========================================
# MODULE 3: RENAME EXECUTOR (Disk Write)
# ==========================================
class RenameExecutor:
    """Handles safe renaming with sanitization and locking."""
    def __init__(self):
        self._rename_lock = threading.Lock() # Global lock for atomic check-and-rename

    def sanitize_name(self, name: str) -> str:
        """Removes characters invalid on Windows/NAS/Linux."""
        return re.sub(INVALID_FILENAME_CHARS, '', name).strip()

    def get_unique_path(self, base_path: str) -> str:
        if not os.path.exists(base_path): return base_path
        counter = 1
        while True:
            new_path = f"{base_path} ({counter})"
            if not os.path.exists(new_path): return new_path
            counter += 1

    def execute(self, old_path: str, new_name: str, live_mode: bool) -> Tuple[str, str]:
        """
        Returns: (status, final_path_name)
        Status: 'renamed', 'dry_run', 'error', 'unchanged'
        """
        parent_dir = os.path.dirname(old_path)
        safe_name = self.sanitize_name(new_name)
        target_base_path = os.path.join(parent_dir, safe_name)

        if os.path.basename(old_path) == safe_name:
            return 'unchanged', safe_name

        # CRITICAL SECTION: Atomic Check & Rename
        # Only one thread can determine uniqueness and rename at a time
        if live_mode:
            with self._rename_lock:
                final_path = self.get_unique_path(target_base_path)
                
                try:
                    # Final safety check
                    if not os.path.exists(old_path): 
                        return 'error', "Source missing"
                    
                    os.rename(old_path, final_path)
                    return 'renamed', os.path.basename(final_path)
                except OSError as e:
                    return 'error', str(e)
        else:
            # Dry Run doesn't need strict locking but logic mirrors it
            final_path = self.get_unique_path(target_base_path)
            return 'dry_run', os.path.basename(final_path)

# ==========================================
# MODULE 4: ORCHESTRATOR
# ==========================================
class MediaFolderOrganizer:
    def __init__(self, args):
        self.target_path = os.path.abspath(args.path)
        self.live_run = args.live
        self.workers = args.workers
        self.confidence = args.confidence
        self.case = args.case
        
        # Init Components
        self.scanner = MetadataScanner()
        self.executor = RenameExecutor()
        self.ai = AIService(args.ai_api_key) if args.ai_api_key else None

    def _clean_base_name(self, name: str) -> str:
        clean = re.sub(r"^[\d\.\-\/\s]+", "", name).strip()
        if not clean: return name
        if self.case == 'upper': return clean.upper()
        if self.case == 'lower': return clean.lower()
        return clean.title()

    def _process_folder(self, folder_path: str) -> Dict[str, Any]:
        folder_name = os.path.basename(folder_path)
        result = {'status': 'skipped', 'name': folder_name, 'reason': '', 'new_name': ''}

        # 1. SCAN
        dates, total = self.scanner.scan_folder(folder_path)
        if not dates:
            result['reason'] = "No metadata"
            return result
            
        # 2. DECIDE
        top_date, count = dates.most_common(1)[0]
        conf = count / total
        if conf < self.confidence:
            result['reason'] = f"Low confidence ({conf:.2f})"
            return result

        # 3. PLAN NAME
        base_name = self._clean_base_name(folder_name)
        if self.ai:
            base_name = self.ai.fix_spelling(base_name, self.case)
        
        new_name_candidate = f"{top_date} {base_name}"

        # 4. EXECUTE
        status, final_name = self.executor.execute(folder_path, new_name_candidate, self.live_run)
        
        result['status'] = status
        result['new_name'] = final_name
        if status == 'error': result['reason'] = final_name # final_name contains error msg here
        
        return result

    def run(self):
        logger.info(f"üöÄ Starting NAS Organizer V24")
        logger.info(f"üìÇ Target: {self.target_path}")
        logger.info(f"üõ†Ô∏è  Mode: {'LIVE' if self.live_run else 'DRY RUN'}")
        logger.info(f"üßµ Workers: {self.workers}")
        
        subdirs = sorted([f.path for f in os.scandir(self.target_path) if f.is_dir() 
                         and os.path.basename(f.path) not in IGNORED_DIRS])
        
        stats = Counter()
        skipped_details = []

        with ThreadPoolExecutor(max_workers=self.workers) as pool:
            futures = {pool.submit(self._process_folder, f): f for f in subdirs}
            
            for fut in tqdm(as_completed(futures), total=len(subdirs), unit="dir"):
                try:
                    res = fut.result()
                    stats[res['status']] += 1
                    
                    if res['status'] == 'renamed':
                        logger.info(f"‚úÖ {res['name']} -> {res['new_name']}")
                    elif res['status'] == 'dry_run':
                        logger.info(f"üîÆ {res['name']} -> {res['new_name']}")
                    elif res['status'] == 'error':
                        logger.error(f"‚ùå {res['name']}: {res['reason']}")
                    elif res['status'] == 'skipped':
                        skipped_details.append(f"{res['name']} ({res['reason']})")
                        
                except Exception as e:
                    logger.critical(f"Worker Crash: {e}")
                    stats['crash'] += 1

        print("\n" + "="*40)
        print("          FINAL REPORT")
        print("="*40)
        print(f"üìÇ Total Scanned : {len(subdirs)}")
        print(f"‚úÖ Renamed       : {stats['renamed']}")
        print(f"üîÆ Dry Run       : {stats['dry_run']}")
        print(f"üí§ Unchanged     : {stats['unchanged']}")
        print(f"‚è≠Ô∏è  Skipped       : {stats['skipped']}")
        print(f"‚ùå Errors        : {stats['error']}")
        
        if skipped_details:
            print("-" * 40)
            print("‚ö†Ô∏è  Skipped Samples:")
            for s in skipped_details[:5]: print(f" ‚Ä¢ {s}")
        print("="*40 + "\n")

# ==========================================
# MAIN ENTRY POINT
# ==========================================
def main():
    parser = argparse.ArgumentParser(description="NAS Photo Organizer V24")
    parser.add_argument("path", help="Folder path")
    parser.add_argument("--live", action="store_true", help="Enable renaming")
    parser.add_argument("--workers", type=int, default=4, help="Default: 4 (NAS Safe)")
    parser.add_argument("--confidence", type=float, default=0.6)
    parser.add_argument("--case", default='upper', choices=['title', 'upper', 'lower'])
    parser.add_argument("--ai-api-key", help="Gemini API Key")
    parser.add_argument("--debug", action="store_true", help="Enable verbose logs")

    args = parser.parse_args()
    
    # Configure Logging (File only for debug/info to save console spam)
    lvl = logging.DEBUG if args.debug else logging.INFO
    logging.basicConfig(filename="exif-nas.log", filemode='w', level=lvl,
                        format="%(asctime)s - %(levelname)s - %(message)s")
    
    # Console gets cleaner output via print/tqdm, minimal logs
    console = logging.StreamHandler()
    console.setLevel(logging.WARNING) # Only warn/error on console to keep tqdm clean
    logging.getLogger('').addHandler(console)

    if not os.path.exists(args.path):
        print("Error: Path not found.")
        return

    MediaFolderOrganizer(args).run()

if __name__ == "__main__":
    main()
